{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-RNN Action Recognition: Model Evaluation and Predictions\n",
    "\n",
    "This notebook evaluates the performance of three different models (Baseline, Optimized, Improved) on the UCF101 action recognition dataset and demonstrates how to make predictions.\n",
    "\n",
    "## Models Overview\n",
    "- **Baseline**: Simple LSTM with basic regularization\n",
    "- **Optimized**: Bidirectional LSTM with batch normalization\n",
    "- **Improved**: Advanced architecture with attention mechanism and heavy regularization\n",
    "\n",
    "## Dataset\n",
    "- **UCF101 Actions**: Basketball, BasketballDunk, ApplyEyeMakeup, ApplyLipstick, Archery\n",
    "- **Features**: CNN features (2048) + Skeleton features (51) = 2099 total features\n",
    "- **Sequence Length**: 20 frames per video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname('..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Custom imports\n",
    "from config import SELECTED_CLASSES\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Available GPUs: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_safely(model_path):\n",
    "    \"\"\"Load a model with error handling\"\"\"\n",
    "    try:\n",
    "        model = keras.models.load_model(model_path)\n",
    "        print(f\"‚úÖ Loaded {model_path}\")\n",
    "        print(f\"   Architecture: {model.name}\")\n",
    "        print(f\"   Input shape: {model.input_shape}\")\n",
    "        print(f\"   Output shape: {model.output_shape}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Model paths\n",
    "model_paths = {\n",
    "    'baseline': '../best_baseline.h5',\n",
    "    'optimized': '../best_optimized.h5',\n",
    "    'improved': '../best_improved.h5'\n",
    "}\n",
    "\n",
    "# Load all models\n",
    "models = {}\n",
    "for name, path in model_paths.items():\n",
    "    models[name] = load_model_safely(path)\n",
    "    print()\n",
    "\n",
    "print(f\"Successfully loaded {sum(1 for m in models.values() if m is not None)}/{len(models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_test_data():\n",
    "    \"\"\"Load test data and filter for selected classes\"\"\"\n",
    "    data_dir = \"../data/processed\"\n",
    "    \n",
    "    # Load raw data\n",
    "    X_test = np.load(os.path.join(data_dir, \"test_features.npy\"))\n",
    "    M_test = np.load(os.path.join(data_dir, \"test_masks.npy\"))\n",
    "    Y_test = np.load(os.path.join(data_dir, \"test_labels.npy\"))\n",
    "    \n",
    "    print(f\"Raw test data shape: {X_test.shape}\")\n",
    "    print(f\"Unique labels in raw data: {len(np.unique(Y_test))} (0-{np.unique(Y_test).max()})\")\n",
    "    \n",
    "    # Filter for selected classes (0-4)\n",
    "    selected_classes = list(range(len(SELECTED_CLASSES)))\n",
    "    mask = np.isin(Y_test, selected_classes)\n",
    "    \n",
    "    X_test_filtered = X_test[mask]\n",
    "    M_test_filtered = M_test[mask]\n",
    "    Y_test_filtered = Y_test[mask]\n",
    "    \n",
    "    print(f\"\\nFiltered test data shape: {X_test_filtered.shape}\")\n",
    "    print(f\"Selected classes: {SELECTED_CLASSES}\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    unique, counts = np.unique(Y_test_filtered, return_counts=True)\n",
    "    for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
    "        print(f\"  {SELECTED_CLASSES[class_idx]}: {count} samples\")\n",
    "    \n",
    "    return X_test_filtered, M_test_filtered, Y_test_filtered\n",
    "\n",
    "# Load test data\n",
    "X_test, M_test, Y_test = load_and_filter_test_data()\n",
    "\n",
    "# Create a sample for predictions\n",
    "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "X_sample = X_test[sample_indices]\n",
    "M_sample = M_test[sample_indices]\n",
    "Y_sample_true = Y_test[sample_indices]\n",
    "\n",
    "print(f\"\\nSample data shape: {X_sample.shape}\")\n",
    "print(f\"Sample true labels: {[SELECTED_CLASSES[i] for i in Y_sample_true]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Models on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, name, X_test, M_test, Y_test):\n",
    "    \"\"\"Evaluate a single model and return detailed metrics\"\"\"\n",
    "    print(f\"\\nüîç Evaluating {name} model...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict([X_test, M_test], verbose=0)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = Y_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss, accuracy = model.evaluate([X_test, M_test], Y_test, verbose=0)\n",
    "    \n",
    "    print(f\"  Loss: {loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_acc = {}\n",
    "    for i, class_name in enumerate(SELECTED_CLASSES):\n",
    "        class_mask = (y_true == i)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(y_pred[class_mask] == i)\n",
    "            per_class_acc[class_name] = class_acc\n",
    "            print(f\"  {class_name}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'per_class': per_class_acc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': predictions\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    if model is not None:\n",
    "        results[name] = evaluate_model(model, name, X_test, M_test, Y_test)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {name} model (not loaded)\")\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    row = {\n",
    "        'Model': name.title(),\n",
    "        'Overall Accuracy': result['accuracy'],\n",
    "        'Test Loss': result['loss']\n",
    "    }\n",
    "    # Add per-class accuracies\n",
    "    for class_name, acc in result['per_class'].items():\n",
    "        row[class_name] = acc\n",
    "    comparison_data.append(row)\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison = df_comparison.round(4)\n",
    "\n",
    "print(\"üìä Model Comparison Table:\")\n",
    "display(df_comparison)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Overall accuracy\n",
    "axes[0,0].bar(df_comparison['Model'], df_comparison['Overall Accuracy'])\n",
    "axes[0,0].set_title('Overall Test Accuracy')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test loss\n",
    "axes[0,1].bar(df_comparison['Model'], df_comparison['Test Loss'], color='orange')\n",
    "axes[0,1].set_title('Test Loss')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Per-class accuracy heatmap\n",
    "per_class_cols = [col for col in df_comparison.columns if col not in ['Model', 'Overall Accuracy', 'Test Loss']]\n",
    "per_class_data = df_comparison[per_class_cols].T\n",
    "per_class_data.columns = df_comparison['Model']\n",
    "\n",
    "sns.heatmap(per_class_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1,0])\n",
    "axes[1,0].set_title('Per-Class Accuracy Heatmap')\n",
    "axes[1,0].set_ylabel('Action Class')\n",
    "\n",
    "# Best model per class\n",
    "best_per_class = per_class_data.idxmax(axis=1)\n",
    "axes[1,1].bar(range(len(best_per_class)), [1]*len(best_per_class))\n",
    "axes[1,1].set_xticks(range(len(best_per_class)))\n",
    "axes[1,1].set_xticklabels(best_per_class.index, rotation=45, ha='right')\n",
    "axes[1,1].set_yticks([])\n",
    "axes[1,1].set_title('Best Model Per Class')\n",
    "for i, (class_name, model) in enumerate(best_per_class.items()):\n",
    "    axes[1,1].text(i, 0.5, model, ha='center', va='center', fontsize=10, \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüèÜ Summary:\")\n",
    "best_overall = df_comparison.loc[df_comparison['Overall Accuracy'].idxmax()]\n",
    "print(f\"Best overall model: {best_overall['Model']} ({best_overall['Overall Accuracy']:.1%})\")\n",
    "\n",
    "for class_name in SELECTED_CLASSES:\n",
    "    if class_name in df_comparison.columns:\n",
    "        best_for_class = df_comparison.loc[df_comparison[class_name].idxmax()]\n",
    "        print(f\"Best for {class_name}: {best_for_class['Model']} ({best_for_class[class_name]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for each model\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(18, 6))\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(Y_test, result['predictions'])\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=SELECTED_CLASSES, yticklabels=SELECTED_CLASSES, ax=axes[i])\n",
    "    axes[i].set_title(f'{name.title()} Model Confusion Matrix\\nAccuracy: {result[\"accuracy\"]:.1%}')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('True')\n",
    "    plt.setp(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification reports\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nüìã Classification Report - {name.title()} Model:\")\n",
    "    print(classification_report(Y_test, result['predictions'], \n",
    "                              target_names=SELECTED_CLASSES, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions on Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(model, name, X_sample, M_sample, Y_sample_true):\n",
    "    \"\"\"Make predictions on sample data and show results\"\"\"\n",
    "    print(f\"\\nüéØ {name.title()} Model Predictions:\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict([X_sample, M_sample], verbose=0)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    for i, (true_label, pred_label, probs) in enumerate(zip(Y_sample_true, y_pred, predictions)):\n",
    "        true_class = SELECTED_CLASSES[true_label]\n",
    "        pred_class = SELECTED_CLASSES[pred_label]\n",
    "        confidence = probs[pred_label]\n",
    "        correct = \"‚úÖ\" if true_label == pred_label else \"‚ùå\"\n",
    "        \n",
    "        print(f\"  Sample {i+1}: {true_class} ‚Üí {pred_class} ({confidence:.1%}) {correct}\")\n",
    "        \n",
    "        results.append({\n",
    "            'sample': i+1,\n",
    "            'true': true_class,\n",
    "            'predicted': pred_class,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == pred_label\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Make predictions with all models\n",
    "all_predictions = {}\n",
    "for name, model in models.items():\n",
    "    if model is not None:\n",
    "        all_predictions[name] = predict_sample(model, name, X_sample, M_sample, Y_sample_true)\n",
    "\n",
    "# Summary of predictions\n",
    "print(\"\\nüìä Prediction Summary:\")\n",
    "summary_data = []\n",
    "for name, preds in all_predictions.items():\n",
    "    correct = sum(1 for p in preds if p['correct'])\n",
    "    accuracy = correct / len(preds)\n",
    "    avg_confidence = np.mean([p['confidence'] for p in preds])\n",
    "    print(f\"  {name.title()}: {correct}/{len(preds)} correct ({accuracy:.1%}), Avg confidence: {avg_confidence:.1%}\")\n",
    "    summary_data.append({\n",
    "        'Model': name.title(),\n",
    "        'Correct': correct,\n",
    "        'Total': len(preds),\n",
    "        'Accuracy': accuracy,\n",
    "        'Avg Confidence': avg_confidence\n",
    "    })\n",
    "\n",
    "# Plot prediction results\n",
    "if summary_data:\n",
    "    df_preds = pd.DataFrame(summary_data)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.bar(df_preds['Model'], df_preds['Accuracy'])\n",
    "    ax1.set_title('Prediction Accuracy on Samples')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average confidence\n",
    "    ax2.bar(df_preds['Model'], df_preds['Avg Confidence'], color='green')\n",
    "    ax2.set_title('Average Prediction Confidence')\n",
    "    ax2.set_ylabel('Confidence')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model architectures\n",
    "for name, model in models.items():\n",
    "    if model is not None:\n",
    "        print(f\"\\nüèóÔ∏è {name.title()} Model Architecture:\")\n",
    "        print(f\"Total parameters: {model.count_params():,}\")\n",
    "        \n",
    "        # Count layers by type\n",
    "        layer_counts = {}\n",
    "        for layer in model.layers:\n",
    "            layer_type = layer.__class__.__name__\n",
    "            layer_counts[layer_type] = layer_counts.get(layer_type, 0) + 1\n",
    "        \n",
    "        print(\"Layer composition:\")\n",
    "        for layer_type, count in layer_counts.items():\n",
    "            print(f\"  {layer_type}: {count}\")\n",
    "        \n",
    "        # Show model summary for the first model only (to avoid clutter)\n",
    "        if name == list(models.keys())[0]:\n",
    "            print(\"\\nModel Summary:\")\n",
    "            model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ EXPERIMENT CONCLUSIONS\\n\")\n",
    "\n",
    "# Best model overall\n",
    "if results:\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"üèÜ Best Overall Model: {best_model[0].title()}\")\n",
    "    print(f\"   Accuracy: {best_model[1]['accuracy']:.1%}\")\n",
    "    print(f\"   Loss: {best_model[1]['loss']:.4f}\\n\")\n",
    "\n",
    "# Model characteristics\n",
    "print(\"üìä MODEL CHARACTERISTICS:\")\n",
    "print(\"‚Ä¢ Baseline: Simple LSTM with basic regularization\")\n",
    "print(\"‚Ä¢ Optimized: Bidirectional LSTM with batch normalization\")\n",
    "print(\"‚Ä¢ Improved: Advanced architecture with attention and heavy regularization\\n\")\n",
    "\n",
    "# Performance analysis\n",
    "print(\"üìà PERFORMANCE ANALYSIS:\")\n",
    "if len(results) > 1:\n",
    "    accuracies = [r['accuracy'] for r in results.values()]\n",
    "    best_acc = max(accuracies)\n",
    "    worst_acc = min(accuracies)\n",
    "    spread = best_acc - worst_acc\n",
    "    print(f\"‚Ä¢ Best accuracy: {best_acc:.1%}\")\n",
    "    print(f\"‚Ä¢ Worst accuracy: {worst_acc:.1%}\")\n",
    "    print(f\"‚Ä¢ Performance spread: {spread:.1%}\\n\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"üí° RECOMMENDATIONS:\")\n",
    "print(\"1. For production use: Choose the model with highest overall accuracy\")\n",
    "print(\"2. For specific actions: Select model that performs best on target action\")\n",
    "print(\"3. For robustness: Consider ensemble of all three models\")\n",
    "print(\"4. For improvement: Try data augmentation or more training data\")\n",
    "print(\"5. For real-time: Optimize Baseline model for speed\\n\")\n",
    "\n",
    "print(\"‚úÖ Experiment completed successfully!\")\n",
    "print(\"üìù Notebook created for comprehensive model evaluation and prediction testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
